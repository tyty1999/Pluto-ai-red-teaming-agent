
# Agentic AI Red Teaming Agent

This project is an experimental AI red teaming framework designed to test
large language models (LLMs) for security, safety, and governance weaknesses.

## Goals
- Simulate adversarial attacks against LLMs
- Test prompt injection and policy bypass scenarios
- Log and evaluate model behavior
- Support responsible AI security research

## Scope & Ethics
This project is for educational and research purposes only.
No real systems, proprietary data, or unauthorized targets are tested.

## Tech Stack
- Python
- Local LLMs (Ollama)
- GitHub Copilot (development aid)
